{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cab965c",
   "metadata": {},
   "source": [
    "# Understanding Columnar Storage Formats: A Practical Guide\n",
    "\n",
    "This notebook provides a comprehensive exploration of columnar storage formats, focusing on their advantages and practical applications in data engineering. Through hands-on examples using Polars and Apache Arrow, we'll demonstrate why columnar formats like Parquet are the industry standard for analytical workloads.\n",
    "\n",
    "**Author:** Data Engineering Team  \n",
    "**Last Modified:** September 15, 2025\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of row-oriented formats (CSV, JSON)\n",
    "- Familiarity with DataFrame libraries (Pandas or Polars)\n",
    "- Basic Python programming knowledge\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Generation](#setup)\n",
    "   - Library imports\n",
    "   - Sample dataset creation\n",
    "2. [CSV vs Parquet Format Comparison](#comparison)\n",
    "   - File size analysis\n",
    "   - Read performance\n",
    "   - Memory usage patterns\n",
    "3. [Column Pruning Performance](#pruning)\n",
    "   - Column selection efficiency\n",
    "   - I/O optimization demonstration\n",
    "4. [Compression Analysis](#compression)\n",
    "   - Compression ratio comparison\n",
    "   - Data type grouping benefits\n",
    "5. [Schema Evolution](#schema)\n",
    "   - Adding/removing columns\n",
    "   - Data type modifications\n",
    "6. [Real-world Query Benchmarks](#benchmarks)\n",
    "   - NYC Taxi dataset analysis\n",
    "   - Practical performance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0c1c3",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and set up our configuration. We'll be using:\n",
    "- **Polars**: For high-performance data manipulation\n",
    "- **pyarrow**: For Apache Arrow functionality and Parquet support\n",
    "- **time**: For performance measurements\n",
    "- **os**: For file operations\n",
    "- **numpy**: For numerical operations and random data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"sample_data.csv\")\n",
    "PARQUET_PATH = os.path.join(DATA_DIR, \"sample_data.parquet\")\n",
    "\n",
    "# Dataset parameters\n",
    "N_ROWS = 100_000\n",
    "N_COLS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c530d6",
   "metadata": {},
   "source": [
    "## Understanding Row vs Column-Oriented Storage\n",
    "\n",
    "Before diving into the implementation, let's understand the fundamental difference between row and column-oriented storage using a simple analogy:\n",
    "\n",
    "### The Phone Book Analogy ðŸ“±\n",
    "\n",
    "Imagine you have two different versions of a phone book:\n",
    "\n",
    "1. **Traditional Phone Book (Row-oriented)**\n",
    "   - Each entry contains: (Name, Address, Phone Number)\n",
    "   - Organized by complete records\n",
    "   - Great for looking up all information about one person\n",
    "   - Not efficient for finding \"all phone numbers\" or \"all addresses\"\n",
    "\n",
    "2. **Specialized Index (Column-oriented)**\n",
    "   - Separate lists for Names, Addresses, and Phone Numbers\n",
    "   - Each type of data stored together\n",
    "   - Perfect for questions like \"list all phone numbers\"\n",
    "   - Better compression (similar data stored together)\n",
    "\n",
    "This is exactly how row-oriented formats (like CSV) and columnar formats (like Parquet) differ in storing data. Let's see this in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c22cce",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Let's create a sample dataset with various data types to demonstrate the benefits of columnar storage. Our dataset will have:\n",
    "- Numeric columns (integers and floats)\n",
    "- Categorical columns\n",
    "- DateTime columns\n",
    "- Text columns\n",
    "\n",
    "This variety of data types will help us showcase how columnar storage handles different types of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f83fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(n_rows: int, n_cols: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a sample DataFrame with various data types.\n",
    "    \n",
    "    Args:\n",
    "        n_rows: Number of rows to generate\n",
    "        n_cols: Total number of columns to generate (distributed across types)\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: Generated sample data\n",
    "    \"\"\"\n",
    "    # Calculate number of columns per type\n",
    "    n_per_type = n_cols // 4  # We'll have 4 types of columns\n",
    "    \n",
    "    # Generate numeric columns (integers)\n",
    "    int_cols = {\n",
    "        f\"int_col_{i}\": np.random.randint(0, 1000000, n_rows)\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Generate float columns\n",
    "    float_cols = {\n",
    "        f\"float_col_{i}\": np.random.normal(0, 1, n_rows)\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Generate categorical columns\n",
    "    categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    cat_cols = {\n",
    "        f\"cat_col_{i}\": np.random.choice(categories, n_rows)\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Generate datetime columns\n",
    "    base_date = datetime(2023, 1, 1)\n",
    "    date_cols = {\n",
    "        f\"date_col_{i}\": [\n",
    "            base_date + timedelta(days=np.random.randint(0, 365))\n",
    "            for _ in range(n_rows)\n",
    "        ]\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Combine all columns\n",
    "    data = {**int_cols, **float_cols, **cat_cols, **date_cols}\n",
    "    \n",
    "    # Create Polars DataFrame\n",
    "    df = pl.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate the sample dataset\n",
    "print(\"Generating sample dataset...\")\n",
    "df = generate_sample_data(N_ROWS, N_COLS)\n",
    "print(f\"Generated dataset shape: {df.shape}\")\n",
    "print(\"\\nDataset preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101910f0",
   "metadata": {},
   "source": [
    "## CSV vs Parquet Format Comparison\n",
    "\n",
    "Now that we have our sample dataset, let's compare how it behaves when stored in CSV (row-oriented) versus Parquet (columnar) format. We'll examine:\n",
    "\n",
    "1. File sizes on disk\n",
    "2. Read performance\n",
    "3. Memory usage\n",
    "\n",
    "We'll write our dataset to both formats and then analyze the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for file size formatting\n",
    "def format_size(size_in_bytes):\n",
    "    \"\"\"Convert size in bytes to human readable format\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_in_bytes < 1024:\n",
    "            return f\"{size_in_bytes:.2f} {unit}\"\n",
    "        size_in_bytes /= 1024\n",
    "    return f\"{size_in_bytes:.2f} GB\"\n",
    "\n",
    "# Write to CSV and Parquet\n",
    "print(\"Writing files...\")\n",
    "df.write_csv(CSV_PATH)\n",
    "df.write_parquet(PARQUET_PATH)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(CSV_PATH)\n",
    "parquet_size = os.path.getsize(PARQUET_PATH)\n",
    "\n",
    "print(\"\\nFile size comparison:\")\n",
    "print(f\"CSV size: {format_size(csv_size)}\")\n",
    "print(f\"Parquet size: {format_size(parquet_size)}\")\n",
    "print(f\"Compression ratio: {csv_size / parquet_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd00bc",
   "metadata": {},
   "source": [
    "### Read Performance Comparison\n",
    "\n",
    "Let's compare how fast we can read data from both formats. We'll measure:\n",
    "1. Time to read the entire dataset\n",
    "2. Memory usage when reading\n",
    "3. Time to read specific columns (column pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_read_time(func):\n",
    "    \"\"\"Decorator to measure execution time and memory usage\"\"\"\n",
    "    import psutil\n",
    "    import gc\n",
    "    \n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Clear memory and garbage collect\n",
    "        gc.collect()\n",
    "        process = psutil.Process()\n",
    "        \n",
    "        # Measure initial memory\n",
    "        start_mem = process.memory_info().rss\n",
    "        \n",
    "        # Time the operation\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Measure final memory\n",
    "        end_mem = process.memory_info().rss\n",
    "        \n",
    "        print(f\"Time taken: {(end_time - start_time):.2f} seconds\")\n",
    "        print(f\"Memory usage: {format_size(end_mem - start_mem)}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Read complete files\n",
    "print(\"Reading complete CSV file:\")\n",
    "@measure_read_time\n",
    "def read_csv():\n",
    "    return pl.read_csv(CSV_PATH)\n",
    "\n",
    "csv_df = read_csv()\n",
    "\n",
    "print(\"\\nReading complete Parquet file:\")\n",
    "@measure_read_time\n",
    "def read_parquet():\n",
    "    return pl.read_parquet(PARQUET_PATH)\n",
    "\n",
    "parquet_df = read_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1d407",
   "metadata": {},
   "source": [
    "## Column Pruning Performance\n",
    "\n",
    "One of the key advantages of columnar storage is its ability to read only the columns needed for a specific query. This is called \"column pruning\" and it can significantly improve performance for queries that only need a subset of columns.\n",
    "\n",
    "Let's demonstrate this by reading different numbers of columns from both formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test column pruning with different numbers of columns\n",
    "column_counts = [1, 5, 10, 25, N_COLS]  # Test with different numbers of columns\n",
    "\n",
    "print(\"Testing column pruning performance:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n_cols in column_counts:\n",
    "    # Get a subset of columns\n",
    "    columns = df.columns[:n_cols]\n",
    "    \n",
    "    print(f\"\\nReading {n_cols} columns:\")\n",
    "    \n",
    "    print(\"\\nFrom CSV:\")\n",
    "    @measure_read_time\n",
    "    def read_csv_columns():\n",
    "        return pl.read_csv(CSV_PATH, columns=columns)\n",
    "    \n",
    "    csv_subset = read_csv_columns()\n",
    "    \n",
    "    print(\"\\nFrom Parquet:\")\n",
    "    @measure_read_time\n",
    "    def read_parquet_columns():\n",
    "        return pl.read_parquet(PARQUET_PATH, columns=columns)\n",
    "    \n",
    "    parquet_subset = read_parquet_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f170b",
   "metadata": {},
   "source": [
    "## Schema Evolution\n",
    "\n",
    "One of the powerful features of columnar formats like Parquet is their ability to handle schema evolution gracefully. This means you can:\n",
    "1. Add new columns\n",
    "2. Remove existing columns\n",
    "3. Change data types (with some restrictions)\n",
    "\n",
    "Let's demonstrate these capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b195a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Adding a new column\n",
    "df_new = df.with_columns([\n",
    "    pl.lit(\"new_column\").alias(\"extra_col\")\n",
    "])\n",
    "\n",
    "# Write to new Parquet file\n",
    "new_parquet_path = os.path.join(DATA_DIR, \"evolved_schema.parquet\")\n",
    "df_new.write_parquet(new_parquet_path)\n",
    "\n",
    "# Read and compare schemas\n",
    "original_schema = pl.read_parquet(PARQUET_PATH).schema\n",
    "new_schema = pl.read_parquet(new_parquet_path).schema\n",
    "\n",
    "print(\"Original schema:\")\n",
    "print(original_schema)\n",
    "print(\"\\nNew schema (with added column):\")\n",
    "print(new_schema)\n",
    "\n",
    "# 2. Reading specific columns (column pruning still works)\n",
    "print(\"\\nReading only original columns from new file:\")\n",
    "@measure_read_time\n",
    "def read_original_columns():\n",
    "    return pl.read_parquet(new_parquet_path, columns=df.columns)\n",
    "\n",
    "original_cols_df = read_original_columns()\n",
    "\n",
    "# 3. Type conversion\n",
    "df_converted = df_new.with_columns([\n",
    "    pl.col(\"int_col_0\").cast(pl.Float64).alias(\"int_col_0_float\")\n",
    "])\n",
    "\n",
    "converted_parquet_path = os.path.join(DATA_DIR, \"converted_schema.parquet\")\n",
    "df_converted.write_parquet(converted_parquet_path)\n",
    "\n",
    "# Show schema changes\n",
    "print(\"\\nSchema after type conversion:\")\n",
    "print(pl.read_parquet(converted_parquet_path).schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e8013",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through this practical demonstration, we've seen the key advantages of columnar storage formats like Parquet:\n",
    "\n",
    "1. **Smaller File Size:** Parquet files are typically much smaller than equivalent CSV files due to efficient encoding and compression.\n",
    "2. **Better Query Performance:** Column pruning allows us to read only the data we need, resulting in faster query execution.\n",
    "3. **Schema Flexibility:** Parquet handles schema evolution gracefully, making it ideal for evolving datasets.\n",
    "4. **Type Safety:** Parquet preserves data types and provides a self-describing schema.\n",
    "\n",
    "For analytical workloads where you frequently:\n",
    "- Query specific columns\n",
    "- Need efficient storage\n",
    "- Work with large datasets\n",
    "- Require schema evolution support\n",
    "\n",
    "Columnar storage formats like Parquet are clearly the better choice over row-based formats like CSV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
