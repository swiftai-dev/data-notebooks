{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cab965c",
   "metadata": {},
   "source": [
    "# Understanding Columnar Storage Formats: A Practical Guide\n",
    "\n",
    "This notebook provides a comprehensive exploration of columnar storage formats, focusing on their advantages and practical applications in data engineering. Through hands-on examples using Polars and Apache Arrow, we'll demonstrate why columnar formats like Parquet are the industry standard for analytical workloads.\n",
    "\n",
    "**Author:** Data Engineering Team  \n",
    "**Last Modified:** September 15, 2025\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of row-oriented formats (CSV, JSON)\n",
    "- Familiarity with DataFrame libraries (Pandas or Polars)\n",
    "- Basic Python programming knowledge\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Generation](#setup)\n",
    "   - Library imports\n",
    "   - Sample dataset creation\n",
    "2. [CSV vs Parquet Format Comparison](#comparison)\n",
    "   - File size analysis\n",
    "   - Read performance\n",
    "   - Memory usage patterns\n",
    "3. [Column Pruning Performance](#pruning)\n",
    "   - Column selection efficiency\n",
    "   - I/O optimization demonstration\n",
    "4. [Compression Analysis](#compression)\n",
    "   - Compression ratio comparison\n",
    "   - Data type grouping benefits\n",
    "5. [Schema Evolution](#schema)\n",
    "   - Adding/removing columns\n",
    "   - Data type modifications\n",
    "6. [Real-world Query Benchmarks](#benchmarks)\n",
    "   - NYC Taxi dataset analysis\n",
    "   - Practical performance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0c1c3",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and set up our configuration. We'll be using:\n",
    "- **Polars**: For high-performance data manipulation\n",
    "- **pyarrow**: For Apache Arrow functionality and Parquet support\n",
    "- **time**: For performance measurements\n",
    "- **os**: For file operations\n",
    "- **numpy**: For numerical operations and random data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e246dd",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f530b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install \".[base, polars]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e4ce6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8982eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# File paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "CSV_PATH = DATA_DIR / \"sample_data.csv\"\n",
    "PARQUET_PATH = DATA_DIR / \"sample_data.parquet\"\n",
    "\n",
    "# Dataset parameters\n",
    "N_ROWS = 100_000\n",
    "N_COLS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c530d6",
   "metadata": {},
   "source": [
    "## Understanding Row vs Column-Oriented Storage\n",
    "\n",
    "Before diving into the implementation, let's understand the fundamental difference between row and column-oriented storage using a simple analogy:\n",
    "\n",
    "### The Phone Book Analogy 📱\n",
    "\n",
    "Imagine you have two different versions of a phone book:\n",
    "\n",
    "1. **Traditional Phone Book (Row-oriented)**\n",
    "   - Each entry contains: (Name, Address, Phone Number)\n",
    "   - Organized by complete records\n",
    "   - Great for looking up all information about one person\n",
    "   - Not efficient for finding \"all phone numbers\" or \"all addresses\"\n",
    "\n",
    "2. **Specialized Index (Column-oriented)**\n",
    "   - Separate lists for Names, Addresses, and Phone Numbers\n",
    "   - Each type of data stored together\n",
    "   - Perfect for questions like \"list all phone numbers\"\n",
    "   - Better compression (similar data stored together)\n",
    "\n",
    "This is exactly how row-oriented formats (like CSV) and columnar formats (like Parquet) differ in storing data. Let's see this in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c22cce",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Let's create a sample dataset with various data types to demonstrate the benefits of columnar storage. Our dataset will have:\n",
    "- Numeric columns (integers and floats)\n",
    "- Categorical columns\n",
    "- DateTime columns\n",
    "- Text columns\n",
    "\n",
    "This variety of data types will help us showcase how columnar storage handles different types of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f83fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample dataset...\n",
      "Generated dataset shape: (100000, 48)\n",
      "\n",
      "Dataset preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 48)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>int_col_0</th><th>int_col_1</th><th>int_col_2</th><th>int_col_3</th><th>int_col_4</th><th>int_col_5</th><th>int_col_6</th><th>int_col_7</th><th>int_col_8</th><th>int_col_9</th><th>int_col_10</th><th>int_col_11</th><th>float_col_0</th><th>float_col_1</th><th>float_col_2</th><th>float_col_3</th><th>float_col_4</th><th>float_col_5</th><th>float_col_6</th><th>float_col_7</th><th>float_col_8</th><th>float_col_9</th><th>float_col_10</th><th>float_col_11</th><th>cat_col_0</th><th>cat_col_1</th><th>cat_col_2</th><th>cat_col_3</th><th>cat_col_4</th><th>cat_col_5</th><th>cat_col_6</th><th>cat_col_7</th><th>cat_col_8</th><th>cat_col_9</th><th>cat_col_10</th><th>cat_col_11</th><th>date_col_0</th><th>date_col_1</th><th>date_col_2</th><th>date_col_3</th><th>date_col_4</th><th>date_col_5</th><th>date_col_6</th><th>date_col_7</th><th>date_col_8</th><th>date_col_9</th><th>date_col_10</th><th>date_col_11</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>121958</td><td>245279</td><td>201413</td><td>581575</td><td>256279</td><td>607614</td><td>228105</td><td>192391</td><td>508457</td><td>814701</td><td>410705</td><td>578370</td><td>0.525842</td><td>-0.468028</td><td>-0.298357</td><td>1.833869</td><td>-0.835418</td><td>-1.903113</td><td>-1.206475</td><td>-0.820048</td><td>0.745167</td><td>1.60712</td><td>-0.08369</td><td>0.611347</td><td>&quot;A&quot;</td><td>&quot;I&quot;</td><td>&quot;B&quot;</td><td>&quot;A&quot;</td><td>&quot;J&quot;</td><td>&quot;I&quot;</td><td>&quot;D&quot;</td><td>&quot;B&quot;</td><td>&quot;I&quot;</td><td>&quot;E&quot;</td><td>&quot;I&quot;</td><td>&quot;J&quot;</td><td>2023-06-28 00:00:00</td><td>2023-11-16 00:00:00</td><td>2023-08-16 00:00:00</td><td>2023-12-28 00:00:00</td><td>2023-04-04 00:00:00</td><td>2023-11-29 00:00:00</td><td>2023-10-19 00:00:00</td><td>2023-11-23 00:00:00</td><td>2023-11-30 00:00:00</td><td>2023-06-26 00:00:00</td><td>2023-05-06 00:00:00</td><td>2023-11-30 00:00:00</td></tr><tr><td>671155</td><td>178356</td><td>359215</td><td>423821</td><td>975578</td><td>59029</td><td>511718</td><td>890571</td><td>771742</td><td>936108</td><td>202453</td><td>368175</td><td>-0.296441</td><td>1.518756</td><td>0.197392</td><td>-0.1359</td><td>-0.464159</td><td>1.477194</td><td>0.448394</td><td>0.046258</td><td>1.334597</td><td>-0.376881</td><td>1.158888</td><td>-1.449445</td><td>&quot;A&quot;</td><td>&quot;C&quot;</td><td>&quot;H&quot;</td><td>&quot;H&quot;</td><td>&quot;E&quot;</td><td>&quot;A&quot;</td><td>&quot;H&quot;</td><td>&quot;F&quot;</td><td>&quot;B&quot;</td><td>&quot;D&quot;</td><td>&quot;B&quot;</td><td>&quot;E&quot;</td><td>2023-05-15 00:00:00</td><td>2023-04-12 00:00:00</td><td>2023-09-09 00:00:00</td><td>2023-12-23 00:00:00</td><td>2023-05-26 00:00:00</td><td>2023-02-27 00:00:00</td><td>2023-03-22 00:00:00</td><td>2023-12-26 00:00:00</td><td>2023-11-12 00:00:00</td><td>2023-03-24 00:00:00</td><td>2023-06-13 00:00:00</td><td>2023-03-08 00:00:00</td></tr><tr><td>131932</td><td>752233</td><td>19752</td><td>871751</td><td>713522</td><td>32471</td><td>872191</td><td>938080</td><td>43188</td><td>997191</td><td>244294</td><td>307420</td><td>-0.364775</td><td>-2.893514</td><td>0.898982</td><td>-0.515307</td><td>1.037464</td><td>2.622034</td><td>-0.640263</td><td>-0.072445</td><td>0.598887</td><td>-0.565942</td><td>0.133276</td><td>0.813489</td><td>&quot;C&quot;</td><td>&quot;E&quot;</td><td>&quot;A&quot;</td><td>&quot;F&quot;</td><td>&quot;E&quot;</td><td>&quot;A&quot;</td><td>&quot;C&quot;</td><td>&quot;A&quot;</td><td>&quot;A&quot;</td><td>&quot;H&quot;</td><td>&quot;H&quot;</td><td>&quot;E&quot;</td><td>2023-05-11 00:00:00</td><td>2023-05-15 00:00:00</td><td>2023-10-26 00:00:00</td><td>2023-03-05 00:00:00</td><td>2023-09-26 00:00:00</td><td>2023-05-21 00:00:00</td><td>2023-10-12 00:00:00</td><td>2023-04-14 00:00:00</td><td>2023-04-13 00:00:00</td><td>2023-09-09 00:00:00</td><td>2023-10-16 00:00:00</td><td>2023-06-21 00:00:00</td></tr><tr><td>365838</td><td>895983</td><td>131058</td><td>438563</td><td>118669</td><td>953547</td><td>365077</td><td>338394</td><td>33506</td><td>261780</td><td>925517</td><td>410270</td><td>-1.616431</td><td>0.071071</td><td>0.606215</td><td>0.214326</td><td>-0.962773</td><td>-1.317677</td><td>-0.799817</td><td>0.591787</td><td>-0.172468</td><td>0.341324</td><td>0.303579</td><td>0.749992</td><td>&quot;A&quot;</td><td>&quot;I&quot;</td><td>&quot;C&quot;</td><td>&quot;I&quot;</td><td>&quot;B&quot;</td><td>&quot;I&quot;</td><td>&quot;B&quot;</td><td>&quot;B&quot;</td><td>&quot;G&quot;</td><td>&quot;H&quot;</td><td>&quot;F&quot;</td><td>&quot;I&quot;</td><td>2023-07-01 00:00:00</td><td>2023-09-15 00:00:00</td><td>2023-05-28 00:00:00</td><td>2023-11-15 00:00:00</td><td>2023-05-02 00:00:00</td><td>2023-04-25 00:00:00</td><td>2023-06-16 00:00:00</td><td>2023-06-15 00:00:00</td><td>2023-10-11 00:00:00</td><td>2023-06-19 00:00:00</td><td>2023-03-02 00:00:00</td><td>2023-05-09 00:00:00</td></tr><tr><td>259178</td><td>63724</td><td>663550</td><td>720980</td><td>952984</td><td>307704</td><td>331649</td><td>687036</td><td>503917</td><td>998094</td><td>712215</td><td>624442</td><td>1.046499</td><td>1.004912</td><td>0.574196</td><td>-0.391352</td><td>1.990713</td><td>-1.239422</td><td>-0.103856</td><td>0.391309</td><td>-0.384701</td><td>0.168152</td><td>-1.219548</td><td>-0.904327</td><td>&quot;D&quot;</td><td>&quot;F&quot;</td><td>&quot;H&quot;</td><td>&quot;A&quot;</td><td>&quot;E&quot;</td><td>&quot;H&quot;</td><td>&quot;H&quot;</td><td>&quot;B&quot;</td><td>&quot;B&quot;</td><td>&quot;C&quot;</td><td>&quot;D&quot;</td><td>&quot;I&quot;</td><td>2023-09-29 00:00:00</td><td>2023-10-10 00:00:00</td><td>2023-10-23 00:00:00</td><td>2023-04-25 00:00:00</td><td>2023-10-16 00:00:00</td><td>2023-04-06 00:00:00</td><td>2023-12-31 00:00:00</td><td>2023-11-12 00:00:00</td><td>2023-08-06 00:00:00</td><td>2023-09-23 00:00:00</td><td>2023-02-16 00:00:00</td><td>2023-03-23 00:00:00</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 48)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ int_col_0 ┆ int_col_1 ┆ int_col_2 ┆ int_col_3 ┆ … ┆ date_col_ ┆ date_col_ ┆ date_col_ ┆ date_col │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ 8         ┆ 9         ┆ 10        ┆ _11      │\n",
       "│ i64       ┆ i64       ┆ i64       ┆ i64       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ datetime[ ┆ datetime[ ┆ datetime[ ┆ datetime │\n",
       "│           ┆           ┆           ┆           ┆   ┆ μs]       ┆ μs]       ┆ μs]       ┆ [μs]     │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 121958    ┆ 245279    ┆ 201413    ┆ 581575    ┆ … ┆ 2023-11-3 ┆ 2023-06-2 ┆ 2023-05-0 ┆ 2023-11- │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0         ┆ 6         ┆ 6         ┆ 30       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00 │\n",
       "│ 671155    ┆ 178356    ┆ 359215    ┆ 423821    ┆ … ┆ 2023-11-1 ┆ 2023-03-2 ┆ 2023-06-1 ┆ 2023-03- │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 2         ┆ 4         ┆ 3         ┆ 08       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00 │\n",
       "│ 131932    ┆ 752233    ┆ 19752     ┆ 871751    ┆ … ┆ 2023-04-1 ┆ 2023-09-0 ┆ 2023-10-1 ┆ 2023-06- │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3         ┆ 9         ┆ 6         ┆ 21       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00 │\n",
       "│ 365838    ┆ 895983    ┆ 131058    ┆ 438563    ┆ … ┆ 2023-10-1 ┆ 2023-06-1 ┆ 2023-03-0 ┆ 2023-05- │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 1         ┆ 9         ┆ 2         ┆ 09       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00 │\n",
       "│ 259178    ┆ 63724     ┆ 663550    ┆ 720980    ┆ … ┆ 2023-08-0 ┆ 2023-09-2 ┆ 2023-02-1 ┆ 2023-03- │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 6         ┆ 3         ┆ 6         ┆ 23       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00  ┆ 00:00:00 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sample_data(n_rows: int, n_cols: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a sample DataFrame with various data types.\n",
    "    \n",
    "    Args:\n",
    "        n_rows: Number of rows to generate\n",
    "        n_cols: Total number of columns to generate (distributed across types)\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: Generated sample data\n",
    "    \"\"\"\n",
    "    # Calculate number of columns per type\n",
    "    n_per_type = n_cols // 4  # We'll have 4 types of columns\n",
    "    \n",
    "    # Generate numeric columns (integers)\n",
    "    int_cols = {\n",
    "        f\"int_col_{i}\": np.random.randint(0, 1000000, n_rows)\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Generate float columns\n",
    "    float_cols = {\n",
    "        f\"float_col_{i}\": np.random.normal(0, 1, n_rows)\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Generate categorical columns\n",
    "    categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    cat_cols = {\n",
    "        f\"cat_col_{i}\": np.random.choice(categories, n_rows)\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Generate datetime columns\n",
    "    base_date = datetime(2023, 1, 1)\n",
    "    date_cols = {\n",
    "        f\"date_col_{i}\": [\n",
    "            base_date + timedelta(days=np.random.randint(0, 365))\n",
    "            for _ in range(n_rows)\n",
    "        ]\n",
    "        for i in range(n_per_type)\n",
    "    }\n",
    "    \n",
    "    # Combine all columns\n",
    "    data = {**int_cols, **float_cols, **cat_cols, **date_cols}\n",
    "    \n",
    "    # Create Polars DataFrame\n",
    "    df = pl.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate the sample dataset\n",
    "print(\"Generating sample dataset...\")\n",
    "df = generate_sample_data(N_ROWS, N_COLS)\n",
    "print(f\"Generated dataset shape: {df.shape}\")\n",
    "print(\"\\nDataset preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101910f0",
   "metadata": {},
   "source": [
    "## CSV vs Parquet Format Comparison\n",
    "\n",
    "Now that we have our sample dataset, let's compare how it behaves when stored in CSV (row-oriented) versus Parquet (columnar) format. We'll examine:\n",
    "\n",
    "1. File sizes on disk\n",
    "2. Read performance\n",
    "3. Memory usage\n",
    "\n",
    "We'll write our dataset to both formats and then analyze the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d311bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.util.profile import format_size\n",
    "\n",
    "# Write to CSV and Parquet\n",
    "print(\"Writing files...\")\n",
    "df.write_csv(CSV_PATH)\n",
    "df.write_parquet(PARQUET_PATH)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = Path(CSV_PATH).stat().st_size\n",
    "parquet_size = Path(PARQUET_PATH).stat().st_size\n",
    "\n",
    "print(\"\\nFile size comparison:\")\n",
    "print(f\"CSV size: {format_size(csv_size)}\")\n",
    "print(f\"Parquet size: {format_size(parquet_size)}\")\n",
    "print(f\"Compression ratio: {csv_size / parquet_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd00bc",
   "metadata": {},
   "source": [
    "### Read Performance Comparison\n",
    "\n",
    "Let's compare how fast we can read data from both formats. We'll measure:\n",
    "1. Time to read the entire dataset\n",
    "2. Memory usage when reading\n",
    "3. Time to read specific columns (column pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fe0ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading complete CSV file:\n",
      "Time taken: 0.587 seconds\n",
      "Memory usage: 169.50 MB\n",
      "\n",
      "Reading complete Parquet file:\n",
      "Time taken: 0.255 seconds\n",
      "Memory usage: 54.21 MB\n"
     ]
    }
   ],
   "source": [
    "from notebooks.util.profile import measure_read_time\n",
    "\n",
    "# Read complete files\n",
    "print(\"Reading complete CSV file:\")\n",
    "@measure_read_time\n",
    "def read_csv():\n",
    "    return pl.read_csv(CSV_PATH)\n",
    "\n",
    "csv_df = read_csv()\n",
    "\n",
    "print(\"\\nReading complete Parquet file:\")\n",
    "@measure_read_time\n",
    "def read_parquet():\n",
    "    return pl.read_parquet(PARQUET_PATH)\n",
    "\n",
    "parquet_df = read_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1d407",
   "metadata": {},
   "source": [
    "## Column Pruning Performance\n",
    "\n",
    "One of the key advantages of columnar storage is its ability to read only the columns needed for a specific query. This is called \"column pruning\" and it can significantly improve performance for queries that only need a subset of columns.\n",
    "\n",
    "Let's demonstrate this by reading different numbers of columns from both formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b3a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing column pruning performance:\n",
      "--------------------------------------------------\n",
      "\n",
      "Reading 1 columns:\n",
      "\n",
      "From CSV:\n",
      "Time taken: 0.097 seconds\n",
      "Memory usage: 62.60 MB\n",
      "\n",
      "From Parquet:\n",
      "Time taken: 0.008 seconds\n",
      "Memory usage: 1.89 MB\n",
      "\n",
      "Reading 5 columns:\n",
      "\n",
      "From CSV:\n",
      "Time taken: 0.094 seconds\n",
      "Memory usage: 66.16 MB\n",
      "\n",
      "From Parquet:\n",
      "Time taken: 0.021 seconds\n",
      "Memory usage: 4.93 MB\n",
      "\n",
      "Reading 10 columns:\n",
      "\n",
      "From CSV:\n",
      "Time taken: 0.074 seconds\n",
      "Memory usage: 71.88 MB\n",
      "\n",
      "From Parquet:\n",
      "Time taken: 0.057 seconds\n",
      "Memory usage: 8.36 MB\n",
      "\n",
      "Reading 25 columns:\n",
      "\n",
      "From CSV:\n",
      "Time taken: 0.112 seconds\n",
      "Memory usage: 80.54 MB\n",
      "\n",
      "From Parquet:\n",
      "Time taken: 0.098 seconds\n",
      "Memory usage: 20.93 MB\n",
      "\n",
      "Reading 50 columns:\n",
      "\n",
      "From CSV:\n",
      "Time taken: 0.186 seconds\n",
      "Memory usage: 158.00 MB\n",
      "\n",
      "From Parquet:\n",
      "Time taken: 0.136 seconds\n",
      "Memory usage: 48.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Test column pruning with different numbers of columns\n",
    "column_counts = [1, 5, 10, 25, N_COLS]  # Test with different numbers of columns\n",
    "\n",
    "print(\"Testing column pruning performance:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n_cols in column_counts:\n",
    "    # Get a subset of columns\n",
    "    columns = df.columns[:n_cols]\n",
    "    \n",
    "    print(f\"\\nReading {n_cols} columns:\")\n",
    "    \n",
    "    print(\"\\nFrom CSV:\")\n",
    "    @measure_read_time\n",
    "    def read_csv_columns():\n",
    "        return pl.read_csv(CSV_PATH, columns=columns)\n",
    "    \n",
    "    csv_subset = read_csv_columns()\n",
    "    \n",
    "    print(\"\\nFrom Parquet:\")\n",
    "    @measure_read_time\n",
    "    def read_parquet_columns():\n",
    "        return pl.read_parquet(PARQUET_PATH, columns=columns)\n",
    "    \n",
    "    parquet_subset = read_parquet_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f170b",
   "metadata": {},
   "source": [
    "## Schema Evolution and Compatibility\n",
    "\n",
    "One of the key advantages of columnar formats like Parquet is their ability to handle schema changes gracefully. This is particularly important in real-world scenarios where data structures evolve over time. Let's demonstrate this with a practical example:\n",
    "\n",
    "### Scenario: Evolving E-commerce Order Data\n",
    "\n",
    "Imagine you're working with e-commerce order data that evolves over time:\n",
    "1. Initially, you track basic order information\n",
    "2. Later, you add customer satisfaction scores\n",
    "3. Finally, you need to handle orders with multiple shipping addresses\n",
    "\n",
    "This example will show how Parquet handles these changes seamlessly, while highlighting the challenges with CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b195a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial order data\n",
    "initial_orders = pl.DataFrame({\n",
    "    'order_id': range(1, 6),\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard'],\n",
    "    'price': [1200, 800, 300, 400, 80],\n",
    "    'date': ['2025-01-01', '2025-01-02', '2025-01-02', '2025-01-03', '2025-01-03']\n",
    "})\n",
    "\n",
    "# Save in both formats\n",
    "initial_csv = DATA_DIR / \"orders_v1.csv\"\n",
    "initial_parquet = DATA_DIR / \"orders_v1.parquet\"\n",
    "\n",
    "initial_orders.write_csv(initial_csv)\n",
    "initial_orders.write_parquet(initial_parquet)\n",
    "\n",
    "print(\"Initial schema:\")\n",
    "print(initial_orders.schema)\n",
    "\n",
    "# Phase 2: Add customer satisfaction scores (some missing)\n",
    "updated_orders = initial_orders.with_columns([\n",
    "    pl.Series('satisfaction_score', [5, 4, None, 5, None], dtype=pl.Float32)\n",
    "])\n",
    "\n",
    "# Save updated data\n",
    "updated_csv = DATA_DIR / \"orders_v2.csv\"\n",
    "updated_parquet = DATA_DIR / \"orders_v2.parquet\"\n",
    "\n",
    "updated_orders.write_csv(updated_csv)\n",
    "updated_orders.write_parquet(updated_parquet)\n",
    "\n",
    "print(\"\\nUpdated schema with satisfaction scores:\")\n",
    "print(updated_orders.schema)\n",
    "\n",
    "# Phase 3: Add multiple shipping addresses (nested data)\n",
    "final_orders = updated_orders.with_columns([\n",
    "    pl.Series('shipping_addresses', [\n",
    "        ['Home: 123 Main St'],\n",
    "        ['Work: 456 Corp Ave', 'Home: 789 Side St'],\n",
    "        ['Home: 321 Oak Rd'],\n",
    "        ['Work: 654 Biz Blvd', 'Pickup: Store #12'],\n",
    "        ['Home: 987 Pine St']\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Try to save in CSV (this might be problematic)\n",
    "try:\n",
    "    final_csv = DATA_DIR / \"orders_v3.csv\"\n",
    "    final_orders.write_csv(final_csv)\n",
    "    print(\"\\nCSV writing succeeded (but lists are converted to strings)\")\n",
    "except Exception as e:\n",
    "    print(\"\\nCSV writing failed:\", str(e))\n",
    "\n",
    "# Save in Parquet (handles nested data naturally)\n",
    "final_parquet = DATA_DIR / \"orders_v3.parquet\"\n",
    "final_orders.write_parquet(final_parquet)\n",
    "\n",
    "print(\"\\nFinal schema with nested shipping addresses:\")\n",
    "print(final_orders.schema)\n",
    "\n",
    "# Demonstrate backwards compatibility\n",
    "print(\"\\nReading old data with new schema (Parquet):\")\n",
    "old_with_new_schema = pl.read_parquet(\n",
    "    initial_parquet, columns=final_orders.columns\n",
    ")\n",
    "print(\"\\nMissing columns are handled gracefully:\")\n",
    "print(old_with_new_schema.columns)\n",
    "\n",
    "print(\"\\nReading new data with old schema (column pruning):\")\n",
    "new_with_old_cols = pl.read_parquet(\n",
    "    final_parquet,\n",
    "    columns=['order_id', 'product', 'price', 'date']\n",
    ")\n",
    "print(\"Only requested columns are loaded:\")\n",
    "print(new_with_old_cols.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e8013",
   "metadata": {},
   "source": [
    "## Key Advantages of Columnar Storage for Schema Evolution\n",
    "\n",
    "The example above demonstrates several key advantages of columnar formats like Parquet over row-based formats like CSV:\n",
    "\n",
    "1. **Handling Missing Data**\n",
    "   - When we added satisfaction scores, Parquet efficiently handles NULL values with its built-in null support\n",
    "   - CSV requires special handling and often uses placeholders like empty strings or \"NA\"\n",
    "\n",
    "2. **Nested Data Structures**\n",
    "   - Parquet natively supports complex data types (lists, maps, structs)\n",
    "   - CSV has no native support for nested data - requires serialization (e.g., converting to strings)\n",
    "   - This becomes crucial when your data naturally contains arrays or nested structures\n",
    "\n",
    "3. **Schema Metadata**\n",
    "   - Parquet files contain explicit schema information\n",
    "   - CSV files have no schema - every reader must guess types\n",
    "   - This means Parquet can ensure data consistency across files\n",
    "\n",
    "4. **Backward/Forward Compatibility**\n",
    "   - Old Parquet files remain readable when schema changes\n",
    "   - New columns can be added without breaking existing code\n",
    "   - Readers can request only the columns they understand\n",
    "\n",
    "5. **Performance Impact**\n",
    "   - When schema evolves, Parquet's column pruning still works\n",
    "   - You only read the columns you need, regardless of schema changes\n",
    "   - This maintains query performance even as schemas grow\n",
    "\n",
    "These features make Parquet (and columnar formats in general) much better suited for real-world data applications where requirements and data structures evolve over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
